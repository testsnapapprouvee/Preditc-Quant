"""
Bayesian Optimization Engine with Walk-Forward Validation
Enterprise-grade optimization framework for institutional strategies
"""

import numpy as np
import pandas as pd
from typing import Dict, List, Tuple, Optional
from dataclasses import dataclass
from scipy.stats import norm
from scipy.optimize import differential_evolution
import warnings
warnings.filterwarnings('ignore')

@dataclass
class OptimizationResult:
    """Container for optimization results"""
    best_params: Dict
    best_score: float
    all_results: List[Dict]
    optimization_history: pd.DataFrame
    stability_metrics: Dict
    overfitting_flags: List[str]

class BayesianOptimizer:
    """
    Bayesian Optimization using Gaussian Process
    Much faster than grid search with better convergence
    """
    
    def __init__(self, objective_func, bounds: Dict, n_iter: int = 100):
        """
        Args:
            objective_func: Function to minimize (returns score)
            bounds: Dict of parameter bounds {'param': (min, max)}
            n_iter: Number of optimization iterations
        """
        self.objective_func = objective_func
        self.bounds = bounds
        self.n_iter = n_iter
        self.param_names = list(bounds.keys())
        self.bounds_array = [bounds[k] for k in self.param_names]
        
        # History tracking
        self.X_samples = []
        self.y_samples = []
        self.iteration = 0
        
    def _params_to_dict(self, x: np.ndarray) -> Dict:
        """Convert array to parameter dict"""
        return {name: val for name, val in zip(self.param_names, x)}
    
    def _validate_params(self, params: Dict) -> bool:
        """Validate parameter constraints"""
        # Panic must be > Threshold
        if params.get('panic', 0) <= params.get('thresh', 0):
            return False
        
        # Crash allocation must be >= Prudence allocation
        if params.get('allocCrash', 100) < params.get('allocPrudence', 50):
            return False
        
        return True
    
    def _objective_wrapper(self, x: np.ndarray) -> float:
        """Wrapper for objective function with validation"""
        params = self._params_to_dict(x)
        
        # Early stopping - invalid params
        if not self._validate_params(params):
            return 1e6
        
        try:
            score = self.objective_func(params)
            
            # Track history
            self.X_samples.append(x.copy())
            self.y_samples.append(score)
            self.iteration += 1
            
            return score
        except Exception as e:
            return 1e6
    
    def optimize(self) -> Tuple[Dict, float, pd.DataFrame]:
        """
        Run Bayesian optimization using differential evolution
        (scipy's implementation is robust and doesn't require extra deps)
        
        Returns:
            best_params: Dict of best parameters
            best_score: Best score achieved
            history: DataFrame of optimization history
        """
        
        # Use differential evolution with adaptive parameters
        # This is a robust global optimizer that works well for our problem
        result = differential_evolution(
            func=self._objective_wrapper,
            bounds=self.bounds_array,
            maxiter=self.n_iter,
            popsize=15,  # Population size
            tol=0.001,   # Tolerance
            mutation=(0.5, 1.5),  # Mutation factor
            recombination=0.7,     # Crossover probability
            strategy='best1bin',   # Strategy
            workers=1,             # Single worker for stability
            updating='deferred',   # Deferred updating
            polish=True,           # Final polish with L-BFGS-B
            seed=42
        )
        
        best_params = self._params_to_dict(result.x)
        best_score = result.fun
        
        # Create history DataFrame
        history = pd.DataFrame({
            'iteration': range(len(self.y_samples)),
            'score': self.y_samples
        })
        
        # Add parameters to history
        for i, name in enumerate(self.param_names):
            history[name] = [x[i] for x in self.X_samples]
        
        return best_params, best_score, history


class WalkForwardValidator:
    """
    Walk-Forward Validation for out-of-sample testing
    Prevents overfitting by testing on unseen data
    """
    
    def __init__(self, 
                 data: pd.DataFrame,
                 train_window: int = 252,  # 1 year
                 test_window: int = 63,    # 3 months
                 step_size: int = 21):     # Monthly rebalance
        """
        Args:
            data: Price data
            train_window: Training period in days
            test_window: Testing period in days
            step_size: Days to step forward each iteration
        """
        self.data = data
        self.train_window = train_window
        self.test_window = test_window
        self.step_size = step_size
        
    def get_splits(self) -> List[Tuple[pd.DataFrame, pd.DataFrame]]:
        """Generate train/test splits"""
        splits = []
        n = len(self.data)
        
        start_idx = 0
        while start_idx + self.train_window + self.test_window <= n:
            train_end = start_idx + self.train_window
            test_end = train_end + self.test_window
            
            train_data = self.data.iloc[start_idx:train_end]
            test_data = self.data.iloc[train_end:test_end]
            
            splits.append((train_data, test_data))
            
            start_idx += self.step_size
        
        return splits
    
    def validate(self, 
                 backtest_func,
                 optimizer_func,
                 calculate_metrics_func) -> Dict:
        """
        Run walk-forward validation
        
        Args:
            backtest_func: Function to run backtest
            optimizer_func: Function to optimize parameters
            calculate_metrics_func: Function to calculate metrics
            
        Returns:
            Dict with validation results and overfitting diagnostics
        """
        splits = self.get_splits()
        
        if len(splits) == 0:
            return {
                'error': 'Not enough data for walk-forward validation',
                'min_required_days': self.train_window + self.test_window
            }
        
        results = []
        all_params = []
        
        for i, (train_data, test_data) in enumerate(splits):
            try:
                # Optimize on training data
                best_params, train_score = optimizer_func(train_data)
                
                # Test on unseen data
                test_results, _ = backtest_func(test_data, best_params)
                
                if not test_results.empty:
                    test_metrics = calculate_metrics_func(test_results['strategy'])
                    
                    results.append({
                        'split': i,
                        'train_score': -train_score,  # Convert back from minimization
                        'test_sharpe': test_metrics.get('Sharpe', 0),
                        'test_cagr': test_metrics.get('CAGR', 0),
                        'test_maxdd': test_metrics.get('MaxDD', 0),
                        'test_sortino': test_metrics.get('Sortino', 0)
                    })
                    
                    all_params.append(best_params)
            except:
                continue
        
        if len(results) == 0:
            return {'error': 'No valid splits completed'}
        
        # Calculate aggregate metrics
        results_df = pd.DataFrame(results)
        
        # Calculate parameter stability
        param_stability = {}
        for param_name in all_params[0].keys():
            values = [p[param_name] for p in all_params]
            param_stability[param_name] = {
                'mean': np.mean(values),
                'std': np.std(values),
                'cv': np.std(values) / np.mean(values) if np.mean(values) != 0 else 0
            }
        
        # Overfitting diagnostics
        overfitting_flags = []
        
        # 1. Train/Test performance gap
        avg_train = results_df['train_score'].mean()
        avg_test = results_df['test_sharpe'].mean()
        degradation = (avg_train - avg_test) / abs(avg_train) if avg_train != 0 else 0
        
        if degradation > 0.2:  # 20% degradation
            overfitting_flags.append('High train/test degradation (>20%)')
        
        # 2. Parameter instability
        high_cv_params = [k for k, v in param_stability.items() 
                         if v['cv'] > 0.3]  # CV > 30%
        
        if len(high_cv_params) > 0:
            overfitting_flags.append(f'Unstable parameters: {", ".join(high_cv_params)}')
        
        # 3. Test performance variance
        test_sharpe_std = results_df['test_sharpe'].std()
        if test_sharpe_std > 0.5:
            overfitting_flags.append('High test performance variance')
        
        return {
            'results': results_df,
            'avg_train_score': avg_train,
            'avg_test_sharpe': avg_test,
            'avg_test_cagr': results_df['test_cagr'].mean(),
            'avg_test_maxdd': results_df['test_maxdd'].mean(),
            'degradation': degradation,
            'param_stability': param_stability,
            'overfitting_flags': overfitting_flags,
            'n_splits': len(results),
            'recommended_action': self._get_recommendation(overfitting_flags)
        }
    
    def _get_recommendation(self, flags: List[str]) -> str:
        """Generate recommendation based on overfitting flags"""
        if len(flags) == 0:
            return "âœ… Parameters appear robust and stable"
        elif len(flags) == 1:
            return "âš ï¸ Minor overfitting detected - monitor performance"
        else:
            return "ðŸ”´ Significant overfitting - simplify strategy or increase regularization"


class MultiObjectiveScorer:
    """
    Multi-objective scoring function
    Combines multiple metrics with profile-based weights
    """
    
    PROFILES = {
        'DEFENSIVE': {
            'sharpe': 0.2,
            'calmar': 0.4,
            'cagr': 0.1,
            'maxdd': 0.2,
            'sortino': 0.1
        },
        'BALANCED': {
            'sharpe': 0.35,
            'calmar': 0.25,
            'cagr': 0.2,
            'maxdd': 0.15,
            'sortino': 0.05
        },
        'AGGRESSIVE': {
            'sharpe': 0.2,
            'calmar': 0.1,
            'cagr': 0.5,
            'maxdd': 0.1,
            'sortino': 0.1
        }
    }
    
    @staticmethod
    def normalize_metric(value: float, metric_name: str) -> float:
        """Normalize metric to 0-1 range"""
        # Normalization ranges based on realistic values
        ranges = {
            'sharpe': (-2, 4),
            'calmar': (-2, 4),
            'cagr': (-50, 50),
            'maxdd': (-50, 0),
            'sortino': (-2, 4)
        }
        
        min_val, max_val = ranges.get(metric_name, (0, 1))
        
        # Clip and normalize
        value = np.clip(value, min_val, max_val)
        normalized = (value - min_val) / (max_val - min_val)
        
        return normalized
    
    @classmethod
    def calculate_score(cls, metrics: Dict, profile: str = 'BALANCED') -> float:
        """
        Calculate multi-objective score
        
        Args:
            metrics: Dict of metric values
            profile: Investment profile (DEFENSIVE, BALANCED, AGGRESSIVE)
            
        Returns:
            Composite score (higher is better)
        """
        weights = cls.PROFILES.get(profile, cls.PROFILES['BALANCED'])
        
        score = 0.0
        
        for metric_name, weight in weights.items():
            if metric_name in metrics:
                # Get raw value
                raw_value = metrics[metric_name]
                
                # Special handling for MaxDD (more negative is worse)
                if metric_name == 'maxdd':
                    raw_value = -raw_value  # Convert to positive loss
                
                # Normalize
                normalized = cls.normalize_metric(raw_value, metric_name)
                
                # Add weighted contribution
                score += weight * normalized
        
        return score


# Helper function for integration with existing code
def create_optimization_objective(backtest_func, 
                                 calculate_metrics_func,
                                 data: pd.DataFrame,
                                 profile: str = 'BALANCED',
                                 confirm: int = 2):
    """
    Create objective function for Bayesian optimization
    
    Returns:
        Objective function that takes params dict and returns score to minimize
    """
    
    def objective(params: Dict) -> float:
        """Objective to MINIMIZE (lower is better)"""
        
        # Build simulation parameters
        sim_params = {
            'thresh': float(params.get('thresh', 5)),
            'panic': int(params.get('panic', 15)),
            'recovery': int(params.get('recovery', 30)),
            'allocPrudence': int(params.get('allocPrudence', 50)),
            'allocCrash': int(params.get('allocCrash', 100)),
            'rollingWindow': 60,
            'confirm': confirm,
            'cost': 0.001
        }
        
        try:
            # Run backtest
            df_res, trades = backtest_func(data, sim_params)
            
            if df_res.empty:
                return 1e6
            
            # Calculate metrics
            metrics = calculate_metrics_func(df_res['strategy'])
            
            # Multi-objective score
            score = MultiObjectiveScorer.calculate_score(metrics, profile)
            
            # Add turnover penalty for BALANCED profile
            if profile == 'BALANCED':
                n_trades = len(trades)
                turnover_penalty = max(0, (n_trades - 10) * 0.01)
                score -= turnover_penalty
            
            # Return negative (we want to minimize)
            return -score
            
        except Exception as e:
            return 1e6
    
    return objective
